# Quality Consensus Protocol

**Version**: 1.0
**Date**: June 22, 2025
**Status**: Technical Protocol Specification

## ðŸŽ¯ Overview

The Quality Consensus Protocol is TrustWrapper's third verification layer, providing distributed validation of AI outputs through Byzantine fault-tolerant consensus among multiple validators. This ensures AI predictions are not only mathematically verifiable (ZK) and explainable (XAI) but also meet quality and safety standards through decentralized agreement.

## ðŸ—ï¸ Protocol Architecture

### System Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Client Request                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Consensus Coordinator                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Request   â”‚  â”‚  Validator  â”‚  â”‚   Result   â”‚ â”‚
â”‚  â”‚   Router    â”‚  â”‚   Pool      â”‚  â”‚ Aggregator â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validator 1 â”‚ â”‚ Validator 2 â”‚ â”‚ Validator N â”‚
â”‚  - Quality  â”‚ â”‚  - Safety   â”‚ â”‚  - Domain   â”‚
â”‚  - Safety   â”‚ â”‚  - Quality  â”‚ â”‚  - Quality  â”‚
â”‚  - Domain   â”‚ â”‚  - Domain   â”‚ â”‚  - Safety   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

1. **Consensus Coordinator**: Orchestrates the validation process
2. **Validator Pool**: Set of independent validators with different specializations
3. **Quality Metrics**: Standardized evaluation criteria
4. **Byzantine Fault Tolerance**: Handles malicious or faulty validators

## ðŸ” Byzantine Fault Tolerant Design

### Threat Model

```python
class ThreatModel:
    """Define the consensus threat model"""

    # Byzantine validators
    MAX_BYZANTINE_RATIO = 1/3  # Up to 33% malicious validators

    # Attack types
    ATTACKS = [
        "false_quality_scores",      # Lying about quality
        "selective_validation",      # Validating only certain requests
        "timing_attacks",           # Delaying consensus
        "collusion",               # Multiple validators cooperating
        "sybil_attacks"            # Creating fake validator identities
    ]

    # Network assumptions
    NETWORK_MODEL = "partially_synchronous"  # Eventual message delivery
    MESSAGE_AUTHENTICATION = True            # Cryptographic signatures
```

### PBFT-Based Consensus

```python
from enum import Enum
from dataclasses import dataclass
from typing import Dict, List, Optional
import hashlib
import time

class ConsensusPhase(Enum):
    IDLE = "idle"
    PRE_PREPARE = "pre_prepare"
    PREPARE = "prepare"
    COMMIT = "commit"
    COMPLETED = "completed"

@dataclass
class ValidationRequest:
    request_id: str
    ai_output: Any
    zk_proof: bytes
    xai_explanation: Dict
    timestamp: float
    client_signature: bytes

@dataclass
class ValidationResult:
    validator_id: str
    request_id: str
    quality_score: float
    safety_assessment: Dict
    domain_check: bool
    signature: bytes

class QualityConsensusProtocol:
    def __init__(self, validator_id: str, validators: List[str],
                 f: int):  # f = max byzantine failures
        self.validator_id = validator_id
        self.validators = validators
        self.f = f
        self.n = len(validators)  # n = 3f + 1

        # State management
        self.phase = ConsensusPhase.IDLE
        self.current_request: Optional[ValidationRequest] = None
        self.prepare_votes: Dict[str, List[str]] = {}
        self.commit_votes: Dict[str, List[str]] = {}
        self.completed_requests: Dict[str, ValidationResult] = {}

    def start_consensus(self, request: ValidationRequest):
        """Initiate consensus as primary"""
        if not self.is_primary():
            raise Exception("Only primary can start consensus")

        self.current_request = request
        self.phase = ConsensusPhase.PRE_PREPARE

        # Validate the request locally
        local_result = self.validate_request(request)

        # Broadcast pre-prepare message
        pre_prepare_msg = {
            'type': 'PRE_PREPARE',
            'view': self.current_view,
            'sequence': self.next_sequence(),
            'request': request,
            'validation': local_result,
            'timestamp': time.time()
        }

        self.broadcast(self.sign_message(pre_prepare_msg))

    def handle_pre_prepare(self, message: Dict):
        """Handle pre-prepare from primary"""
        if not self.verify_primary_signature(message):
            return

        if self.phase != ConsensusPhase.IDLE:
            return  # Already processing a request

        # Validate the request
        request = message['request']
        local_result = self.validate_request(request)

        # Move to prepare phase
        self.phase = ConsensusPhase.PREPARE
        self.current_request = request

        # Broadcast prepare message
        prepare_msg = {
            'type': 'PREPARE',
            'view': message['view'],
            'sequence': message['sequence'],
            'request_hash': self.hash_request(request),
            'validation': local_result,
            'validator': self.validator_id
        }

        self.broadcast(self.sign_message(prepare_msg))

    def handle_prepare(self, message: Dict):
        """Handle prepare messages from other validators"""
        if not self.verify_signature(message):
            return

        request_hash = message['request_hash']
        validator = message['validator']

        # Collect prepare votes
        if request_hash not in self.prepare_votes:
            self.prepare_votes[request_hash] = []

        self.prepare_votes[request_hash].append(validator)

        # Check if we have enough prepares (2f + 1)
        if len(self.prepare_votes[request_hash]) >= 2 * self.f + 1:
            self.phase = ConsensusPhase.COMMIT

            # Aggregate validation results
            aggregated_result = self.aggregate_validations(
                self.prepare_votes[request_hash]
            )

            # Broadcast commit message
            commit_msg = {
                'type': 'COMMIT',
                'view': self.current_view,
                'sequence': message['sequence'],
                'request_hash': request_hash,
                'aggregated_result': aggregated_result,
                'validator': self.validator_id
            }

            self.broadcast(self.sign_message(commit_msg))

    def handle_commit(self, message: Dict):
        """Handle commit messages"""
        if not self.verify_signature(message):
            return

        request_hash = message['request_hash']
        validator = message['validator']

        # Collect commit votes
        if request_hash not in self.commit_votes:
            self.commit_votes[request_hash] = []

        self.commit_votes[request_hash].append(validator)

        # Check if we have enough commits (2f + 1)
        if len(self.commit_votes[request_hash]) >= 2 * self.f + 1:
            self.phase = ConsensusPhase.COMPLETED

            # Finalize consensus result
            final_result = self.finalize_consensus(
                self.current_request,
                message['aggregated_result']
            )

            self.completed_requests[request_hash] = final_result

            # Notify client
            self.send_to_client(final_result)

            # Reset for next request
            self.reset_state()
```

## ðŸ“Š Quality Validation Framework

### Multi-Dimensional Validation

```python
class QualityValidator:
    """Comprehensive quality validation for AI outputs"""

    def __init__(self, domain_expertise: str):
        self.domain = domain_expertise
        self.quality_metrics = self.load_quality_metrics()
        self.safety_checker = SafetyChecker()
        self.domain_validator = DomainValidator(domain_expertise)

    def validate_request(self, request: ValidationRequest) -> ValidationResult:
        """Perform comprehensive validation"""
        scores = {}

        # 1. Quality Assessment
        scores['quality'] = self.assess_quality(
            request.ai_output,
            request.xai_explanation
        )

        # 2. Safety Check
        scores['safety'] = self.safety_checker.check(
            request.ai_output,
            request.zk_proof
        )

        # 3. Domain-Specific Validation
        scores['domain'] = self.domain_validator.validate(
            request.ai_output,
            self.domain
        )

        # 4. Consistency Check
        scores['consistency'] = self.check_consistency(
            request.ai_output,
            request.xai_explanation,
            request.zk_proof
        )

        # Aggregate scores
        overall_score = self.aggregate_scores(scores)

        return ValidationResult(
            validator_id=self.validator_id,
            request_id=request.request_id,
            quality_score=overall_score,
            safety_assessment=scores['safety'],
            domain_check=scores['domain'] > 0.8,
            signature=self.sign_result(scores)
        )

    def assess_quality(self, output: Any, explanation: Dict) -> float:
        """Assess output quality"""
        quality_scores = []

        # Confidence score from model
        if 'confidence' in output:
            quality_scores.append(output['confidence'])

        # Explanation coherence
        coherence = self.measure_explanation_coherence(explanation)
        quality_scores.append(coherence)

        # Output validity
        validity = self.check_output_validity(output)
        quality_scores.append(validity)

        # Uncertainty quantification
        uncertainty = self.quantify_uncertainty(output, explanation)
        quality_scores.append(1 - uncertainty)  # Lower uncertainty = higher quality

        return np.mean(quality_scores)

    def measure_explanation_coherence(self, explanation: Dict) -> float:
        """Measure how coherent the explanation is"""
        scores = []

        # Feature importance consistency
        if 'shap_values' in explanation and 'lime_weights' in explanation:
            correlation = np.corrcoef(
                explanation['shap_values'],
                explanation['lime_weights']
            )[0, 1]
            scores.append(abs(correlation))

        # Explanation completeness
        completeness = len(explanation.get('features', [])) / 20  # Assume 20 features
        scores.append(min(completeness, 1.0))

        # Logical consistency
        if 'rules' in explanation:
            consistency = self.check_rule_consistency(explanation['rules'])
            scores.append(consistency)

        return np.mean(scores) if scores else 0.5
```

### Safety Validation

```python
class SafetyChecker:
    """Check AI outputs for safety concerns"""

    def __init__(self):
        self.harmful_patterns = self.load_harmful_patterns()
        self.bias_detector = BiasDetector()
        self.adversarial_detector = AdversarialDetector()

    def check(self, output: Any, proof: bytes) -> Dict[str, Any]:
        """Comprehensive safety check"""
        safety_results = {
            'is_safe': True,
            'concerns': [],
            'risk_score': 0.0
        }

        # 1. Harmful content detection
        harmful_score = self.detect_harmful_content(output)
        if harmful_score > 0.3:
            safety_results['concerns'].append({
                'type': 'harmful_content',
                'severity': harmful_score,
                'details': 'Potentially harmful content detected'
            })
            safety_results['risk_score'] = max(
                safety_results['risk_score'],
                harmful_score
            )

        # 2. Bias detection
        bias_results = self.bias_detector.detect(output)
        if bias_results['bias_detected']:
            safety_results['concerns'].append({
                'type': 'bias',
                'severity': bias_results['severity'],
                'details': bias_results['bias_types']
            })
            safety_results['risk_score'] = max(
                safety_results['risk_score'],
                bias_results['severity']
            )

        # 3. Adversarial detection
        adversarial_score = self.adversarial_detector.detect(output, proof)
        if adversarial_score > 0.5:
            safety_results['concerns'].append({
                'type': 'adversarial',
                'severity': adversarial_score,
                'details': 'Potential adversarial manipulation'
            })
            safety_results['risk_score'] = max(
                safety_results['risk_score'],
                adversarial_score
            )

        # 4. Consistency with proof
        if not self.verify_proof_consistency(output, proof):
            safety_results['concerns'].append({
                'type': 'proof_mismatch',
                'severity': 0.8,
                'details': 'Output inconsistent with ZK proof'
            })
            safety_results['risk_score'] = 0.8

        safety_results['is_safe'] = safety_results['risk_score'] < 0.5
        return safety_results
```

### Domain-Specific Validation

```python
class DomainValidator:
    """Validate outputs against domain-specific requirements"""

    def __init__(self, domain: str):
        self.domain = domain
        self.domain_rules = self.load_domain_rules(domain)
        self.domain_knowledge = self.load_domain_knowledge(domain)

    def validate(self, output: Any, domain: str) -> float:
        """Domain-specific validation"""
        if domain == 'healthcare':
            return self.validate_healthcare(output)
        elif domain == 'finance':
            return self.validate_finance(output)
        elif domain == 'legal':
            return self.validate_legal(output)
        else:
            return self.validate_general(output)

    def validate_healthcare(self, output: Any) -> float:
        """Healthcare-specific validation"""
        scores = []

        # Medical terminology accuracy
        if 'diagnosis' in output:
            term_accuracy = self.check_medical_terms(output['diagnosis'])
            scores.append(term_accuracy)

        # Treatment recommendation safety
        if 'treatment' in output:
            safety_score = self.check_treatment_safety(output['treatment'])
            scores.append(safety_score)

        # Regulatory compliance (HIPAA, FDA)
        compliance_score = self.check_healthcare_compliance(output)
        scores.append(compliance_score)

        # Clinical guidelines adherence
        if 'recommendations' in output:
            guideline_score = self.check_clinical_guidelines(
                output['recommendations']
            )
            scores.append(guideline_score)

        return np.mean(scores) if scores else 0.0

    def validate_finance(self, output: Any) -> float:
        """Financial domain validation"""
        scores = []

        # Regulatory compliance (SEC, FINRA)
        compliance_score = self.check_financial_compliance(output)
        scores.append(compliance_score)

        # Risk assessment accuracy
        if 'risk_score' in output:
            risk_validity = self.validate_risk_assessment(output['risk_score'])
            scores.append(risk_validity)

        # Market manipulation checks
        manipulation_score = 1 - self.detect_market_manipulation(output)
        scores.append(manipulation_score)

        return np.mean(scores) if scores else 0.0
```

## ðŸ”„ Consensus Mechanisms

### 1. Weighted Voting

```python
class WeightedConsensus:
    """Consensus with validator reputation weights"""

    def __init__(self, validators: List[Validator]):
        self.validators = validators
        self.reputation_scores = self.load_reputation_scores()

    def aggregate_votes(self, votes: List[ValidationResult]) -> Dict:
        """Aggregate votes with reputation weighting"""
        weighted_scores = []
        total_weight = 0

        for vote in votes:
            validator_id = vote.validator_id
            weight = self.reputation_scores.get(validator_id, 0.5)

            weighted_scores.append({
                'quality': vote.quality_score * weight,
                'safety': vote.safety_assessment['risk_score'] * weight,
                'domain': float(vote.domain_check) * weight,
                'weight': weight
            })
            total_weight += weight

        # Normalize by total weight
        aggregated = {
            'quality': sum(s['quality'] for s in weighted_scores) / total_weight,
            'safety': sum(s['safety'] for s in weighted_scores) / total_weight,
            'domain': sum(s['domain'] for s in weighted_scores) / total_weight,
            'consensus_strength': total_weight / len(self.validators)
        }

        return aggregated

    def update_reputation(self, validator_id: str, performance: float):
        """Update validator reputation based on performance"""
        current = self.reputation_scores.get(validator_id, 0.5)
        # Exponential moving average
        alpha = 0.1
        new_reputation = alpha * performance + (1 - alpha) * current

        # Bound reputation scores
        self.reputation_scores[validator_id] = np.clip(new_reputation, 0.1, 1.0)
```

### 2. Stake-Based Consensus

```python
class StakeBasedConsensus:
    """Consensus weighted by validator stakes"""

    def __init__(self, stake_registry: StakeRegistry):
        self.stake_registry = stake_registry
        self.slashing_conditions = self.load_slashing_conditions()

    def validate_with_stake(self, validator_id: str,
                          validation: ValidationResult) -> bool:
        """Validate with stake at risk"""
        stake = self.stake_registry.get_stake(validator_id)

        if stake < self.minimum_stake:
            return False  # Insufficient stake

        # Record validation with stake
        self.record_validation(validator_id, validation, stake)

        return True

    def slash_malicious_validator(self, validator_id: str,
                                evidence: Dict):
        """Slash stake for provably malicious behavior"""
        if self.verify_malicious_behavior(evidence):
            stake = self.stake_registry.get_stake(validator_id)
            slash_amount = stake * self.slashing_conditions['malicious_rate']

            self.stake_registry.slash(validator_id, slash_amount)
            self.emit_slashing_event(validator_id, slash_amount, evidence)
```

### 3. Commit-Reveal Consensus

```python
class CommitRevealConsensus:
    """Two-phase consensus preventing vote manipulation"""

    def __init__(self):
        self.commitments = {}
        self.revelations = {}
        self.phase = 'commit'

    def commit_vote(self, validator_id: str, vote_hash: bytes):
        """Commit phase: submit hash of vote"""
        if self.phase != 'commit':
            raise Exception("Not in commit phase")

        self.commitments[validator_id] = {
            'hash': vote_hash,
            'timestamp': time.time()
        }

        # Check if all validators committed
        if len(self.commitments) >= self.required_validators:
            self.phase = 'reveal'
            self.reveal_deadline = time.time() + self.reveal_timeout

    def reveal_vote(self, validator_id: str, vote: ValidationResult,
                   nonce: bytes):
        """Reveal phase: submit actual vote with nonce"""
        if self.phase != 'reveal':
            raise Exception("Not in reveal phase")

        # Verify commitment
        vote_data = self.serialize_vote(vote) + nonce
        vote_hash = hashlib.sha256(vote_data).digest()

        if vote_hash != self.commitments[validator_id]['hash']:
            raise Exception("Vote doesn't match commitment")

        self.revelations[validator_id] = vote

        # Check if consensus reached
        if len(self.revelations) >= self.required_validators:
            return self.finalize_consensus()
```

## ðŸ“ˆ Performance Optimization

### 1. Parallel Validation

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ParallelValidator:
    """Optimize validation through parallelization"""

    def __init__(self, num_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
        self.validation_cache = LRUCache(maxsize=1000)

    async def validate_parallel(self, request: ValidationRequest,
                              validators: List[Validator]) -> List[ValidationResult]:
        """Validate request across multiple validators in parallel"""

        # Check cache first
        cache_key = self.compute_cache_key(request)
        if cache_key in self.validation_cache:
            return self.validation_cache[cache_key]

        # Create validation tasks
        tasks = []
        for validator in validators:
            task = asyncio.create_task(
                self.validate_async(validator, request)
            )
            tasks.append(task)

        # Wait for all validations
        results = await asyncio.gather(*tasks)

        # Cache results
        self.validation_cache[cache_key] = results

        return results

    async def validate_async(self, validator: Validator,
                           request: ValidationRequest) -> ValidationResult:
        """Async wrapper for validation"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            validator.validate_request,
            request
        )
```

### 2. Optimistic Consensus

```python
class OptimisticConsensus:
    """Fast consensus with optimistic assumptions"""

    def __init__(self, threshold: float = 0.9):
        self.threshold = threshold
        self.pending_validations = {}

    def quick_consensus(self, early_votes: List[ValidationResult]) -> Optional[Dict]:
        """Attempt early consensus with partial votes"""
        if len(early_votes) < self.minimum_votes:
            return None

        # Check if early votes agree strongly
        quality_scores = [v.quality_score for v in early_votes]
        mean_quality = np.mean(quality_scores)
        std_quality = np.std(quality_scores)

        # High agreement among early voters
        if std_quality < 0.1 and mean_quality > self.threshold:
            return {
                'consensus': 'optimistic',
                'quality': mean_quality,
                'confidence': 1 - std_quality,
                'validators': len(early_votes)
            }

        return None  # Need full consensus
```

## ðŸ” Security Measures

### 1. Sybil Resistance

```python
class SybilResistance:
    """Prevent Sybil attacks on consensus"""

    def __init__(self):
        self.validator_registry = ValidatorRegistry()
        self.proof_of_work_difficulty = 20  # bits

    def register_validator(self, validator_info: Dict) -> bool:
        """Register new validator with Sybil resistance"""

        # 1. Proof of Work requirement
        if not self.verify_pow(validator_info['pow_nonce'],
                             validator_info['address']):
            return False

        # 2. Stake requirement
        if validator_info['stake'] < self.minimum_stake:
            return False

        # 3. Identity verification (optional)
        if self.require_identity:
            if not self.verify_identity(validator_info['identity_proof']):
                return False

        # 4. Rate limiting
        if self.recent_registrations > self.max_registration_rate:
            return False

        # Register validator
        self.validator_registry.add(validator_info)
        return True

    def verify_pow(self, nonce: int, address: str) -> bool:
        """Verify proof of work"""
        data = f"{address}{nonce}".encode()
        hash_value = hashlib.sha256(data).hexdigest()

        # Check leading zeros
        required_zeros = self.proof_of_work_difficulty // 4
        return hash_value.startswith('0' * required_zeros)
```

### 2. Eclipse Attack Prevention

```python
class EclipseDefense:
    """Prevent eclipse attacks on consensus network"""

    def __init__(self):
        self.peer_connections = {}
        self.peer_diversity_requirement = 0.7

    def validate_peer_connections(self, validator_id: str,
                                peers: List[str]) -> bool:
        """Ensure peer diversity to prevent eclipse"""

        # Geographic diversity
        geo_diversity = self.calculate_geo_diversity(peers)
        if geo_diversity < self.peer_diversity_requirement:
            return False

        # AS (Autonomous System) diversity
        as_diversity = self.calculate_as_diversity(peers)
        if as_diversity < self.peer_diversity_requirement:
            return False

        # Stake distribution
        stake_concentration = self.calculate_stake_concentration(peers)
        if stake_concentration > 0.5:  # No peer group controls >50% stake
            return False

        return True

    def enforce_peer_rotation(self, validator_id: str):
        """Periodically rotate peers to prevent targeted attacks"""
        current_peers = self.peer_connections[validator_id]

        # Keep some stable peers
        stable_peers = random.sample(
            current_peers,
            len(current_peers) // 3
        )

        # Get new random peers
        new_peers = self.get_random_peers(
            exclude=current_peers,
            count=len(current_peers) - len(stable_peers)
        )

        self.peer_connections[validator_id] = stable_peers + new_peers
```

## ðŸ“Š Metrics and Monitoring

### Consensus Health Metrics

```python
class ConsensusMetrics:
    """Monitor consensus protocol health"""

    def __init__(self):
        self.metrics = {
            'consensus_latency': [],
            'validator_participation': {},
            'agreement_scores': [],
            'failed_consensus': 0,
            'total_consensus': 0
        }

    def record_consensus(self, consensus_result: Dict, duration: float):
        """Record consensus metrics"""
        self.metrics['consensus_latency'].append(duration)
        self.metrics['total_consensus'] += 1

        if consensus_result.get('failed'):
            self.metrics['failed_consensus'] += 1
        else:
            agreement = consensus_result['agreement_score']
            self.metrics['agreement_scores'].append(agreement)

        # Update validator participation
        for validator in consensus_result['participants']:
            if validator not in self.metrics['validator_participation']:
                self.metrics['validator_participation'][validator] = 0
            self.metrics['validator_participation'][validator] += 1

    def get_health_report(self) -> Dict:
        """Generate consensus health report"""
        return {
            'average_latency': np.mean(self.metrics['consensus_latency']),
            'p99_latency': np.percentile(self.metrics['consensus_latency'], 99),
            'success_rate': 1 - (self.metrics['failed_consensus'] /
                               self.metrics['total_consensus']),
            'average_agreement': np.mean(self.metrics['agreement_scores']),
            'validator_participation_rate': self.calculate_participation_rate(),
            'recommendations': self.generate_recommendations()
        }
```

## ðŸ”§ Configuration and Tuning

### Consensus Parameters

```yaml
# consensus_config.yaml
consensus:
  # Protocol selection
  protocol: "pbft"  # Options: pbft, raft, tendermint

  # Validator settings
  validators:
    minimum_count: 4
    maximum_count: 100
    byzantine_tolerance: 0.33  # Max fraction of Byzantine validators

  # Timing parameters
  timeouts:
    pre_prepare: 1000  # ms
    prepare: 2000      # ms
    commit: 2000       # ms
    view_change: 5000  # ms

  # Quality thresholds
  quality:
    minimum_score: 0.7
    agreement_threshold: 0.8

  # Performance optimization
  optimization:
    enable_caching: true
    cache_ttl: 300  # seconds
    parallel_validation: true
    max_parallel_validators: 10

  # Security settings
  security:
    require_stake: true
    minimum_stake: 1000  # tokens
    slashing_enabled: true
    sybil_resistance: "proof_of_work"
```

### Dynamic Parameter Adjustment

```python
class DynamicConsensusAdjustment:
    """Dynamically adjust consensus parameters based on network conditions"""

    def __init__(self, config_path: str):
        self.config = self.load_config(config_path)
        self.network_monitor = NetworkMonitor()
        self.performance_history = []

    def adjust_parameters(self):
        """Adjust consensus parameters based on current conditions"""
        network_state = self.network_monitor.get_state()

        # Adjust timeouts based on network latency
        if network_state['average_latency'] > 100:  # High latency
            self.config['timeouts']['prepare'] = 3000
            self.config['timeouts']['commit'] = 3000

        # Adjust validator count based on load
        if network_state['request_rate'] > 1000:  # High load
            self.config['validators']['minimum_count'] = 7
            self.config['optimization']['parallel_validation'] = True

        # Adjust quality thresholds based on attack detection
        if network_state['anomaly_score'] > 0.7:  # Possible attack
            self.config['quality']['minimum_score'] = 0.8
            self.config['quality']['agreement_threshold'] = 0.9

        self.apply_config_changes()
```

---

*For implementation examples and integration guides, see the [TrustWrapper Consensus Examples](https://github.com/trustwrapper/consensus-examples)*
