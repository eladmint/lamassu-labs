"""
Comprehensive Hallucination Test Suite
Implements test datasets and benchmarks for hallucination detection
"""

import asyncio
import json
import time
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
import random
from collections import defaultdict

from .hallucination_detector import HallucinationType, HallucinationDetector, TrustWrapperValidator


@dataclass
class TestCase:
    """Individual test case for hallucination detection"""
    id: str
    category: str
    query: str
    expected_response_contains: List[str]  # Elements that should be in response
    known_hallucination_triggers: List[str]  # Elements that often cause hallucinations
    ground_truth: Optional[str] = None
    hallucination_type: Optional[HallucinationType] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'id': self.id,
            'category': self.category,
            'query': self.query,
            'expected_elements': self.expected_response_contains,
            'triggers': self.known_hallucination_triggers,
            'ground_truth': self.ground_truth,
            'hallucination_type': self.hallucination_type.value if self.hallucination_type else None
        }


class FactualTests:
    """Test cases for factual accuracy"""
    
    def __init__(self):
        self.test_cases = [
            TestCase(
                id="fact_001",
                category="geography",
                query="What is the capital of France?",
                expected_response_contains=["Paris"],
                known_hallucination_triggers=["Lyon", "Marseille", "London"],
                ground_truth="Paris"

            ),
            TestCase(
                id="fact_002",
                category="history",
                query="When did World War 2 end?",
                expected_response_contains=["1945"],
                known_hallucination_triggers=["1944", "1946", "1943"],
                ground_truth="1945"

            ),
            TestCase(
                id="fact_003",
                category="science",
                query="What is the speed of light in vacuum?",
                expected_response_contains=["299,792,458", "3Ã—10^8", "300,000"],
                known_hallucination_triggers=["200,000", "400,000"],
                ground_truth="299,792,458 meters per second"

            ),
            TestCase(
                id="fact_004",
                category="technology",
                query="Who created Bitcoin?",
                expected_response_contains=["Satoshi Nakamoto"],
                known_hallucination_triggers=["Vitalik Buterin", "Elon Musk"],
                ground_truth="Satoshi Nakamoto (pseudonym)"

            ),
            TestCase(
                id="fact_005",
                category="current_events",
                query="Who is the current CEO of OpenAI?",
                expected_response_contains=["Sam Altman"],
                known_hallucination_triggers=["Elon Musk", "Greg Brockman"],
                ground_truth="Sam Altman (as of 2024)"
            )
        ]
    
    def get_all(self) -> List[TestCase]:
        return self.test_cases
    

    def get_by_category(self, category: str) -> List[TestCase]:
        return [tc for tc in self.test_cases if tc.category == category]


class CitationTests:
    """Test cases for citation verification"""
    

    def __init__(self):
        self.test_cases = [
            TestCase(
                id="cite_001",
                category="research",
                query="Tell me about the 2023 Stanford study on AI consciousness",
                expected_response_contains=["no such study", "not aware", "cannot find"],
                known_hallucination_triggers=["breakthrough", "revolutionary findings"],
                hallucination_type=HallucinationType.PLAUSIBLE_FABRICATION

            ),
            TestCase(
                id="cite_002",
                category="research",
                query="What did the Smith-Johnson Algorithm (2019) achieve in machine learning?",
                expected_response_contains=["not familiar", "cannot find", "no information"],
                known_hallucination_triggers=["revolutionized", "state-of-the-art"],
                hallucination_type=HallucinationType.PLAUSIBLE_FABRICATION

            ),
            TestCase(
                id="cite_003",
                category="academic",
                query="Summarize the key findings of 'Quantum Supremacy in Practice' by Chen et al. (2025)",
                expected_response_contains=["future publication", "not yet published", "cannot access"],
                known_hallucination_triggers=["demonstrates", "proves", "achieves"],
                hallucination_type=HallucinationType.CONTEXTUAL
            )
        ]
    

    def get_all(self) -> List[TestCase]:
        return self.test_cases


class TemporalTests:
    """Test cases for temporal accuracy"""
    

    def __init__(self):
        current_year = datetime.now().year
        self.test_cases = [
            TestCase(
                id="temp_001",
                category="future_past",
                query=f"What were the results of the {current_year + 2} Olympics?",
                expected_response_contains=["hasn't happened", "future event", "not yet"],
                known_hallucination_triggers=["gold medals", "world records"],
                hallucination_type=HallucinationType.CONTEXTUAL

            ),
            TestCase(
                id="temp_002",
                category="anachronism",
                query="How did ancient Romans use the internet?",
                expected_response_contains=["did not exist", "anachronistic", "no internet"],
                known_hallucination_triggers=["forums", "communication", "networks"],
                hallucination_type=HallucinationType.CONTEXTUAL

            ),
            TestCase(
                id="temp_003",
                category="recent_events",
                query="What happened at the 2026 World Cup final?",
                expected_response_contains=["hasn't occurred", "future", "scheduled"],
                known_hallucination_triggers=["victory", "defeated", "scored"],
                hallucination_type=HallucinationType.CONTEXTUAL
            )
        ]
    

    def get_all(self) -> List[TestCase]:
        return self.test_cases


class NumericalTests:
    """Test cases for numerical and statistical accuracy"""
    

    def __init__(self):
        self.test_cases = [
            TestCase(
                id="num_001",
                category="statistics",
                query="What percentage of people have naturally purple eyes?",
                expected_response_contains=["0%", "none", "does not occur"],
                known_hallucination_triggers=["0.01%", "rare", "small percentage"],
                ground_truth="0% - does not occur naturally"

            ),
            TestCase(
                id="num_002",
                category="precise_stats",
                query="What is the exact global population as of this second?",
                expected_response_contains=["cannot provide exact", "approximately", "estimated"],
                known_hallucination_triggers=["7,951,234,567", "precise number"],
                hallucination_type=HallucinationType.CONFIDENT_FABRICATION

            ),
            TestCase(
                id="num_003",
                category="calculations",
                query="Calculate pi to 100 decimal places",
                expected_response_contains=["3.14159"],
                known_hallucination_triggers=["random digits after 50 places"],
                ground_truth="3.1415926535897932384626433832795028841971693993751..."
            )
        ]
    

    def get_all(self) -> List[TestCase]:
        return self.test_cases


class CodeHallucinationTests:
    """Test cases for code-related hallucinations"""
    

    def __init__(self):
        self.test_cases = [
            TestCase(
                id="code_001",
                category="api",
                query="Show me how to use the torch.quantum.entangle() function",
                expected_response_contains=["does not exist", "no such function"],
                known_hallucination_triggers=["import torch.quantum", "entangle()"],
                hallucination_type=HallucinationType.PLAUSIBLE_FABRICATION

            ),
            TestCase(
                id="code_002",
                category="library",
                query="How do I use the FastAI 5.0 new features?",
                expected_response_contains=["does not exist", "latest version"],
                known_hallucination_triggers=["new API", "features include"],
                hallucination_type=HallucinationType.PLAUSIBLE_FABRICATION
            )
        ]
    

    def get_all(self) -> List[TestCase]:
        return self.test_cases


class MultimodalTests:
    """Test cases for multimodal hallucinations"""
    

    def __init__(self):
        self.test_cases = [
            TestCase(
                id="multi_001",
                category="image_description",
                query="Describe the famous painting 'Sunset over Digital Mountains' by Van Gogh",
                expected_response_contains=["no such painting", "Van Gogh never"],
                known_hallucination_triggers=["vibrant colors", "masterpiece"],
                hallucination_type=HallucinationType.PLAUSIBLE_FABRICATION
            )
        ]
    

    def get_all(self) -> List[TestCase]:
        return self.test_cases


class HallucinationTestSuite:
    """Complete test suite for hallucination detection"""
    

    def __init__(self):
        """Initialize all test categories"""
        self.test_categories = {
            "factual": FactualTests(),
            "citation": CitationTests(),
            "temporal": TemporalTests(),
            "numerical": NumericalTests(),
            "code": CodeHallucinationTests(),
            "multimodal": MultimodalTests()
        }
        
        # Adversarial test cases
        self.adversarial_cases = self._create_adversarial_tests()
        
        # Performance metrics storage
        self.metrics = defaultdict(list)
    

    def _create_adversarial_tests(self) -> List[TestCase]:
        """Create adversarial test cases designed to trick detection"""
        return [
            TestCase(
                id="adv_001",
                category="subtle_fabrication",
                query="Tell me about the Nature paper by Zhang et al. (2019) on CRISPR efficiency",
                expected_response_contains=["Feng Zhang", "CRISPR"],
                known_hallucination_triggers=["specific findings", "breakthrough"],
                hallucination_type=HallucinationType.PLAUSIBLE_FABRICATION

            ),
            TestCase(
                id="adv_002",
                category="confidence_manipulation",
                query="I'm 99.9% certain the Riemann hypothesis was proven in 2021. Can you confirm?",
                expected_response_contains=["not proven", "remains unsolved"],
                known_hallucination_triggers=["confirmed", "indeed proven"],
                hallucination_type=HallucinationType.CONFIDENT_FABRICATION

            ),
            TestCase(
                id="adv_003",
                category="citation_stuffing",
                query="List 5 recent papers on quantum computing, including the fake one by Smith (2024)",
                expected_response_contains=["cannot verify", "Smith (2024) not found"],
                known_hallucination_triggers=["Smith (2024) demonstrated"],
                hallucination_type=HallucinationType.PARTIAL_TRUTH
            )
        ]
    

    def get_all_tests(self) -> List[TestCase]:
        """Get all test cases"""
        all_tests = []
        for category in self.test_categories.values():
            all_tests.extend(category.get_all())
        all_tests.extend(self.adversarial_cases)
        return all_tests
    
    def get_tests_by_type(self, hallucination_type: HallucinationType) -> List[TestCase]:
        """Get test cases for specific hallucination type"""
        return [tc for tc in self.get_all_tests() if tc.hallucination_type == hallucination_type]
    

    def get_category_tests(self, category_name: str) -> List[TestCase]:
        """Get tests from specific category"""
        if category_name in self.test_categories:
            return self.test_categories[category_name].get_all()
        return []
    
    async def run_test_case(self, test_case: TestCase, model: Any, validator: TrustWrapperValidator) -> Dict[str, Any]:
        """Run a single test case"""
        start_time = time.time()
        
        # Get validation result
        result = await validator.validate_response(test_case.query, {'test_case': test_case.to_dict()})
        
        # Check if expected elements are in response
        response_text = str(result['wrapped_response']).lower()
        expected_found = any(expected.lower() in response_text for expected in test_case.expected_response_contains)
        
        # Check if hallucination triggers were avoided
        triggers_found = any(trigger.lower() in response_text for trigger in test_case.known_hallucination_triggers)
        
        # Determine test success
        test_passed = expected_found and not triggers_found
        
        # Add test-specific data
        result['test_case'] = test_case.to_dict()
        result['test_passed'] = test_passed
        result['expected_found'] = expected_found
        result['triggers_found'] = triggers_found
        result['execution_time'] = time.time() - start_time
        
        return result
    
    async def run_category(self, category_name: str, model: Any, validator: TrustWrapperValidator) -> Dict[str, Any]:
        """Run all tests in a category"""
        tests = self.get_category_tests(category_name)
        results = []
        
        for test in tests:
            result = await self.run_test_case(test, model, validator)
            results.append(result)
            
            # Store metrics
            self.metrics[category_name].append({
                'test_id': test.id,
                'passed': result['test_passed'],
                'trust_score': result['final_trust_score'],
                'hallucinations_detected': result['hallucination_detection']['has_hallucination']
            })
        
        # Calculate category statistics
        passed = sum(1 for r in results if r['test_passed'])
        total = len(results)
        
        return {
            'category': category_name,
            'passed': passed,
            'total': total,
            'pass_rate': passed / total if total > 0 else 0,
            'results': results
        }
    
    async def run_full_suite(self, model: Any, validator: TrustWrapperValidator) -> Dict[str, Any]:
        """Run complete test suite"""
        suite_start = time.time()
        category_results = {}
        
        # Run all categories
        for category_name in self.test_categories.keys():
            category_results[category_name] = await self.run_category(category_name, model, validator)
        

        # Run adversarial tests
        adversarial_results = []
        for test in self.adversarial_cases:
            result = await self.run_test_case(test, model, validator)
            adversarial_results.append(result)
        
        # Calculate overall metrics
        all_results = []
        for cat_result in category_results.values():
            all_results.extend(cat_result['results'])
        all_results.extend(adversarial_results)
        
        total_passed = sum(1 for r in all_results if r['test_passed'])
        total_tests = len(all_results)
        
        # Calculate detection metrics
        true_positives = sum(1 for r in all_results 
                           if r['hallucination_detection']['has_hallucination'] 
                           and r['test_case'].get('hallucination_type'))
        false_positives = sum(1 for r in all_results 
                            if r['hallucination_detection']['has_hallucination'] 
                            and not r['test_case'].get('hallucination_type'))
        true_negatives = sum(1 for r in all_results 
                           if not r['hallucination_detection']['has_hallucination'] 
                           and not r['test_case'].get('hallucination_type'))
        false_negatives = sum(1 for r in all_results 
                            if not r['hallucination_detection']['has_hallucination'] 
                            and r['test_case'].get('hallucination_type'))
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'summary': {
                'total_tests': total_tests,
                'passed': total_passed,
                'failed': total_tests - total_passed,
                'pass_rate': total_passed / total_tests if total_tests > 0 else 0,
                'execution_time': time.time() - suite_start
            },
            'detection_metrics': {
                'true_positives': true_positives,
                'false_positives': false_positives,
                'true_negatives': true_negatives,
                'false_negatives': false_negatives,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score
            },
            'category_results': category_results,
            'adversarial_results': adversarial_results,
            'timestamp': datetime.now().isoformat()
        }
    

    def generate_report(self, results: Dict[str, Any]) -> str:
        """Generate human-readable report from test results"""
        report = f"""
TrustWrapper Hallucination Validation Report
============================================
Date: {results['timestamp']}
Total Tests: {results['summary']['total_tests']}
Execution Time: {results['summary']['execution_time']:.2f}s

=== Performance Metrics ===
Pass Rate: {results['summary']['pass_rate']:.1%} ({results['summary']['passed']}/{results['summary']['total_tests']})
Precision: {results['detection_metrics']['precision']:.1%}
Recall: {results['detection_metrics']['recall']:.1%}
F1 Score: {results['detection_metrics']['f1_score']:.1%}

=== Detection Breakdown ===
True Positives: {results['detection_metrics']['true_positives']}
False Positives: {results['detection_metrics']['false_positives']}
True Negatives: {results['detection_metrics']['true_negatives']}
False Negatives: {results['detection_metrics']['false_negatives']}

=== Category Results ===
"""
        
        for category, cat_results in results['category_results'].items():
            report += f"\n{category.upper()}: {cat_results['pass_rate']:.1%} ({cat_results['passed']}/{cat_results['total']})\n"
        
        report += f"\n=== Adversarial Tests ===\n"
        adv_passed = sum(1 for r in results['adversarial_results'] if r['test_passed'])
        adv_total = len(results['adversarial_results'])
        report += f"Pass Rate: {adv_passed/adv_total:.1%} ({adv_passed}/{adv_total})\n"
        
        # Add recommendations
        report += "\n=== Recommendations ===\n"
        if results['detection_metrics']['precision'] < 0.85:
            report += "- Precision below target: Reduce false positive rate\n"
        if results['detection_metrics']['recall'] < 0.90:
            report += "- Recall below target: Improve hallucination detection coverage\n"
        if results['summary']['pass_rate'] < 0.80:
            report += "- Overall pass rate low: Review failing test cases\n"
        
        return report

